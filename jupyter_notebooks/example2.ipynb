{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/TanayPatare/CONTENT-BASED-MOVIE-RECOMENDER-SYSTEM/raw/main/Data/heart.rar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heart.rar'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"E:\\PROJECTS\\Spark-Project\\jupyter_notebooks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example2.ipynb'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"E:\\PROJECTS\\Spark-Project\\jupyter_notebooks\\heart.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.dataframe_schema import schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification_data_frame = spark.read.csv(path=path1,schema=schema,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| 63|  1|  3|     145| 233|  1|      0|    150|    0|    2.3|    0|  0|   1|     1|\n",
      "| 37|  1|  2|     130| 250|  0|      1|    187|    0|    3.5|    0|  0|   2|     1|\n",
      "| 41|  0|  1|     130| 204|  0|      0|    172|    0|    1.4|    2|  0|   2|     1|\n",
      "| 56|  1|  1|     120| 236|  0|      1|    178|    0|    0.8|    2|  0|   2|     1|\n",
      "| 57|  0|  0|     120| 354|  0|      1|    163|    1|    0.6|    2|  0|   2|     1|\n",
      "| 57|  1|  0|     140| 192|  0|      1|    148|    0|    0.4|    1|  0|   1|     1|\n",
      "| 56|  0|  1|     140| 294|  0|      0|    153|    0|    1.3|    1|  0|   2|     1|\n",
      "| 44|  1|  1|     120| 263|  0|      1|    173|    0|    0.0|    2|  0|   3|     1|\n",
      "| 52|  1|  2|     172| 199|  1|      1|    162|    0|    0.5|    2|  0|   3|     1|\n",
      "| 57|  1|  2|     150| 168|  0|      1|    174|    0|    1.6|    2|  0|   2|     1|\n",
      "| 54|  1|  0|     140| 239|  0|      1|    160|    0|    1.2|    2|  0|   2|     1|\n",
      "| 48|  0|  2|     130| 275|  0|      1|    139|    0|    0.2|    2|  0|   2|     1|\n",
      "| 49|  1|  1|     130| 266|  0|      1|    171|    0|    0.6|    2|  0|   2|     1|\n",
      "| 64|  1|  3|     110| 211|  0|      0|    144|    1|    1.8|    1|  0|   2|     1|\n",
      "| 58|  0|  3|     150| 283|  1|      0|    162|    0|    1.0|    2|  0|   2|     1|\n",
      "| 50|  0|  2|     120| 219|  0|      1|    158|    0|    1.6|    1|  0|   2|     1|\n",
      "| 58|  0|  2|     120| 340|  0|      1|    172|    0|    0.0|    2|  0|   2|     1|\n",
      "| 66|  0|  3|     150| 226|  0|      1|    114|    0|    2.6|    0|  0|   2|     1|\n",
      "| 43|  1|  0|     150| 247|  0|      1|    171|    0|    1.5|    2|  0|   2|     1|\n",
      "| 69|  0|  3|     140| 239|  0|      1|    151|    0|    1.8|    2|  2|   2|     1|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Classification_data_frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|  0|  0|  0|       0|   0|  0|      0|      0|    0|      0|    0|  0|   0|     0|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count,isnan,isnull,when,col\n",
    "Classification_data_frame.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in Classification_data_frame.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = Classification_data_frame.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| 29|  1|  1|     130| 204|  0|      0|    202|    0|    0.0|    2|  0|   2|     1|\n",
      "| 34|  0|  1|     118| 210|  0|      1|    192|    0|    0.7|    2|  0|   2|     1|\n",
      "| 34|  1|  3|     118| 182|  0|      0|    174|    0|    0.0|    2|  0|   2|     1|\n",
      "| 35|  0|  0|     138| 183|  0|      1|    182|    0|    1.4|    2|  0|   2|     1|\n",
      "| 35|  1|  0|     120| 198|  0|      1|    130|    1|    1.6|    1|  0|   3|     0|\n",
      "| 35|  1|  0|     126| 282|  0|      0|    156|    1|    0.0|    2|  0|   3|     0|\n",
      "| 35|  1|  1|     122| 192|  0|      1|    174|    0|    0.0|    2|  0|   2|     1|\n",
      "| 37|  0|  2|     120| 215|  0|      1|    170|    0|    0.0|    2|  0|   2|     1|\n",
      "| 37|  1|  2|     130| 250|  0|      1|    187|    0|    3.5|    0|  0|   2|     1|\n",
      "| 38|  1|  2|     138| 175|  0|      1|    173|    0|    0.0|    2|  4|   2|     1|\n",
      "| 38|  1|  2|     138| 175|  0|      1|    173|    0|    0.0|    2|  4|   2|     1|\n",
      "| 38|  1|  3|     120| 231|  0|      1|    182|    1|    3.8|    1|  0|   3|     0|\n",
      "| 39|  0|  2|      94| 199|  0|      1|    179|    0|    0.0|    2|  0|   2|     1|\n",
      "| 39|  0|  2|     138| 220|  0|      1|    152|    0|    0.0|    1|  0|   2|     1|\n",
      "| 39|  1|  0|     118| 219|  0|      1|    140|    0|    1.2|    1|  0|   3|     0|\n",
      "| 39|  1|  2|     140| 321|  0|      0|    182|    0|    0.0|    2|  0|   2|     1|\n",
      "| 40|  1|  0|     110| 167|  0|      0|    114|    1|    2.0|    1|  0|   3|     0|\n",
      "| 40|  1|  0|     152| 223|  0|      1|    181|    0|    0.0|    2|  0|   3|     0|\n",
      "| 40|  1|  3|     140| 199|  0|      1|    178|    1|    1.4|    2|  0|   3|     1|\n",
      "| 41|  0|  1|     105| 198|  0|      1|    168|    0|    0.0|    2|  1|   2|     1|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 29|\n",
      "| 34|\n",
      "| 34|\n",
      "| 35|\n",
      "| 35|\n",
      "| 35|\n",
      "| 35|\n",
      "| 37|\n",
      "| 37|\n",
      "| 38|\n",
      "| 38|\n",
      "| 38|\n",
      "| 39|\n",
      "| 39|\n",
      "| 39|\n",
      "| 39|\n",
      "| 40|\n",
      "| 40|\n",
      "| 40|\n",
      "| 41|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|target|count|\n",
      "+------+-----+\n",
      "|     0|  111|\n",
      "|     1|  145|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupBy('target').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|target|count|\n",
      "+------+-----+\n",
      "|     1|   20|\n",
      "|     0|   27|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.groupBy('target').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\PROJECTS\\\\Spark-Project\\\\jupyter_notebooks'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/e:/PROJECTS/Spark-Project/jupyter_notebooks/test.csv already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mcsv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49msave(path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39me:\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mPROJECTS\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mSpark-Project\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mjupyter_notebooks\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mtest.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\site-packages\\pyspark\\sql\\readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[0;32m    739\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 740\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\site-packages\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/e:/PROJECTS/Spark-Project/jupyter_notebooks/test.csv already exists."
     ]
    }
   ],
   "source": [
    "test.write.format(\"csv\").save(path='e:\\\\PROJECTS\\\\Spark-Project\\\\jupyter_notebooks\\\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.dataframe_schema import schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = spark.read.csv(\"heart.csv\",header=True,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 'int', 'sex': 'int', 'cp': 'int', 'trestbps': 'int', 'chol': 'int', 'fbs': 'int', 'restecg': 'int', 'thalach': 'int', 'exang': 'int', 'oldpeak': 'float', 'slope': 'int', 'ca': 'int', 'thal': 'int', 'target': 'int'}\n"
     ]
    }
   ],
   "source": [
    "dic = {}\n",
    "for col in data1.dtypes:\n",
    "    dic[col[0]] = col[1]\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(age,IntegerType,true),StructField(sex,IntegerType,true),StructField(cp,IntegerType,true),StructField(trestbps,IntegerType,true),StructField(chol,IntegerType,true),StructField(fbs,IntegerType,true),StructField(restecg,IntegerType,true),StructField(thalach,IntegerType,true),StructField(exang,IntegerType,true),StructField(oldpeak,FloatType,true),StructField(slope,IntegerType,true),StructField(ca,IntegerType,true),StructField(thal,IntegerType,true),StructField(target,IntegerType,true)))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- cp: integer (nullable = true)\n",
      " |-- trestbps: integer (nullable = true)\n",
      " |-- chol: integer (nullable = true)\n",
      " |-- fbs: integer (nullable = true)\n",
      " |-- restecg: integer (nullable = true)\n",
      " |-- thalach: integer (nullable = true)\n",
      " |-- exang: integer (nullable = true)\n",
      " |-- oldpeak: float (nullable = true)\n",
      " |-- slope: integer (nullable = true)\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- thal: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.printSchema(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data1\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data1' is not defined"
     ]
    }
   ],
   "source": [
    "data1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classification.utils.util import read_yaml_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 'int',\n",
       " 'sex': 'int',\n",
       " 'cp': 'int',\n",
       " 'trestbps': 'int',\n",
       " 'chol': 'int',\n",
       " 'fbs': 'int',\n",
       " 'restecg': 'int',\n",
       " 'thalach': 'int',\n",
       " 'exang': 'int',\n",
       " 'oldpeak': 'float',\n",
       " 'slope': 'int',\n",
       " 'ca': 'int',\n",
       " 'thal': 'int',\n",
       " 'target': 'int'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = read_yaml_file(file_path='E:\\PROJECTS\\Spark-Project\\config\\schema.yaml')\n",
    "y = x[\"column\"]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'sex',\n",
       " 'cp',\n",
       " 'trestbps',\n",
       " 'chol',\n",
       " 'fbs',\n",
       " 'restecg',\n",
       " 'thalach',\n",
       " 'exang',\n",
       " 'oldpeak',\n",
       " 'slope',\n",
       " 'ca',\n",
       " 'thal',\n",
       " 'target']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['int',\n",
       " 'int',\n",
       " 'int',\n",
       " 'int',\n",
       " 'int',\n",
       " 'int',\n",
       " 'int',\n",
       " 'int',\n",
       " 'int',\n",
       " 'float',\n",
       " 'int',\n",
       " 'int',\n",
       " 'int',\n",
       " 'int']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dic.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', \"target'\"])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.keys(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if list(dic.values()) == list(y.values()):\n",
    "    x = True\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', \"target'\"])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if list(dic.keys()) == list(y.keys()):\n",
    "    k = True\n",
    "    print(k)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if x and k:\n",
    "    print(True) \n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClassificationException",
     "evalue": "\n        Error occured in script: \n        [ e:\\projects\\spark-project\\Classification\\config\\configuration.py ] at \n        try block line number: [20] and exception block line number: [25] \n        error message: [\n        Error occured in script: \n        [ e:\\projects\\spark-project\\Classification\\utils\\util.py ] at \n        try block line number: [13] and exception block line number: [16] \n        error message: [[Errno 2] No such file or directory: 'config\\\\config.yaml']\n        ]\n        ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32me:\\projects\\spark-project\\Classification\\utils\\util.py:13\u001b[0m, in \u001b[0;36mread_yaml_file\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m yaml_file:\n\u001b[0;32m     14\u001b[0m         \u001b[39mreturn\u001b[39;00m yaml\u001b[39m.\u001b[39msafe_load(yaml_file)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config\\\\config.yaml'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mClassificationException\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32me:\\projects\\spark-project\\Classification\\config\\configuration.py:20\u001b[0m, in \u001b[0;36mConfiguration.__init__\u001b[1;34m(self, config_file_path, current_time_stamp)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_info \u001b[39m=\u001b[39m read_yaml_file(file_path\u001b[39m=\u001b[39;49mconfig_file_path)\n\u001b[0;32m     21\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_pipeline_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_training_pipeline_config()\n",
      "File \u001b[1;32me:\\projects\\spark-project\\Classification\\utils\\util.py:16\u001b[0m, in \u001b[0;36mread_yaml_file\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> 16\u001b[0m     \u001b[39mraise\u001b[39;00m ClassificationException(e,sys) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mClassificationException\u001b[0m: \n        Error occured in script: \n        [ e:\\projects\\spark-project\\Classification\\utils\\util.py ] at \n        try block line number: [13] and exception block line number: [16] \n        error message: [[Errno 2] No such file or directory: 'config\\\\config.yaml']\n        ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mClassificationException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mClassification\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mentity\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig_entity\u001b[39;00m \u001b[39mimport\u001b[39;00m DataValidationConfig\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mClassification\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfiguration\u001b[39;00m \u001b[39mimport\u001b[39;00m Configuration\n\u001b[1;32m----> 3\u001b[0m x \u001b[39m=\u001b[39m Configuration()\u001b[39m.\u001b[39mget_data_validation_config()\n\u001b[0;32m      4\u001b[0m x\n",
      "File \u001b[1;32me:\\projects\\spark-project\\Classification\\config\\configuration.py:25\u001b[0m, in \u001b[0;36mConfiguration.__init__\u001b[1;34m(self, config_file_path, current_time_stamp)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_stamp \u001b[39m=\u001b[39m current_time_stamp\n\u001b[0;32m     24\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> 25\u001b[0m     \u001b[39mraise\u001b[39;00m ClassificationException(e,sys) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mClassificationException\u001b[0m: \n        Error occured in script: \n        [ e:\\projects\\spark-project\\Classification\\config\\configuration.py ] at \n        try block line number: [20] and exception block line number: [25] \n        error message: [\n        Error occured in script: \n        [ e:\\projects\\spark-project\\Classification\\utils\\util.py ] at \n        try block line number: [13] and exception block line number: [16] \n        error message: [[Errno 2] No such file or directory: 'config\\\\config.yaml']\n        ]\n        "
     ]
    }
   ],
   "source": [
    "from Classification.entity.config_entity import DataValidationConfig\n",
    "from Classification.config.configuration import Configuration\n",
    "x = Configuration().get_data_validation_config()\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d56a2e1583cc91001f51b4648654dfeeefc02c3b006ec9b840bc622a1ba33f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
