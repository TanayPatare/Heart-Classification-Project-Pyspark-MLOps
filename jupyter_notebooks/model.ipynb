{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"age\", IntegerType(),nullable=True),\n",
    "    StructField(\"sex\", IntegerType(),nullable=True),\n",
    "    StructField(\"cp\", IntegerType(),nullable=True),\n",
    "    StructField(\"trestbps\", IntegerType(),nullable=True),\n",
    "    StructField(\"chol\", IntegerType(),nullable=True),\n",
    "    StructField(\"fbs\", IntegerType(),nullable=True),\n",
    "    StructField(\"restecg\", IntegerType(),nullable=True),\n",
    "    StructField(\"thalach\", IntegerType(),nullable=True),\n",
    "    StructField(\"exang\", IntegerType(),nullable=True),\n",
    "    StructField(\"oldpeak\", FloatType(),nullable=True),\n",
    "    StructField(\"slope\", IntegerType(),nullable=True),\n",
    "    StructField(\"ca\", IntegerType(),nullable=True),\n",
    "    StructField(\"thal\", IntegerType(),nullable=True),\n",
    "    StructField(\"target\", IntegerType(),nullable=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"heart.csv\"\n",
    "df = spark.read.csv(path=path,schema=schema,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| 63|  1|  3|     145| 233|  1|      0|    150|    0|    2.3|    0|  0|   1|     1|\n",
      "| 37|  1|  2|     130| 250|  0|      1|    187|    0|    3.5|    0|  0|   2|     1|\n",
      "| 41|  0|  1|     130| 204|  0|      0|    172|    0|    1.4|    2|  0|   2|     1|\n",
      "| 56|  1|  1|     120| 236|  0|      1|    178|    0|    0.8|    2|  0|   2|     1|\n",
      "| 57|  0|  0|     120| 354|  0|      1|    163|    1|    0.6|    2|  0|   2|     1|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- cp: integer (nullable = true)\n",
      " |-- trestbps: integer (nullable = true)\n",
      " |-- chol: integer (nullable = true)\n",
      " |-- fbs: integer (nullable = true)\n",
      " |-- restecg: integer (nullable = true)\n",
      " |-- thalach: integer (nullable = true)\n",
      " |-- exang: integer (nullable = true)\n",
      " |-- oldpeak: float (nullable = true)\n",
      " |-- slope: integer (nullable = true)\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- thal: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age   : -0.22543871587483838\n",
      "sex   : -0.28093657550176687\n",
      "cp   : 0.4337982615068946\n",
      "trestbps   : -0.14493112849775\n",
      "chol   : -0.08523910513756904\n",
      "fbs   : -0.02804576027271302\n",
      "restecg   : 0.1372295028737732\n",
      "thalach   : 0.4217409338106742\n",
      "exang   : -0.43675708335330315\n",
      "oldpeak   : -0.4306960030062106\n",
      "slope   : 0.34587707824172464\n",
      "ca   : -0.39172399235125244\n",
      "thal   : -0.34402926803830997\n",
      "target   : 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(i,\" \",\":\",df.stat.corr(i,\"target\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['age', 'trestbps', 'chol', 'thalach', 'oldpeak','sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+-------+-------+---+---+---+-------+-----+-----+---+----+------+\n",
      "|age|trestbps|chol|thalach|oldpeak|sex| cp|fbs|restecg|exang|slope| ca|thal|target|\n",
      "+---+--------+----+-------+-------+---+---+---+-------+-----+-----+---+----+------+\n",
      "| 63|     145| 233|    150|    2.3|  1|  3|  1|      0|    0|    0|  0|   1|     1|\n",
      "| 37|     130| 250|    187|    3.5|  1|  2|  0|      1|    0|    0|  0|   2|     1|\n",
      "| 41|     130| 204|    172|    1.4|  0|  1|  0|      0|    0|    2|  0|   2|     1|\n",
      "| 56|     120| 236|    178|    0.8|  1|  1|  0|      1|    0|    2|  0|   2|     1|\n",
      "| 57|     120| 354|    163|    0.6|  0|  0|  0|      1|    1|    2|  0|   2|     1|\n",
      "+---+--------+----+-------+-------+---+---+---+-------+-----+-----+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = VectorAssembler(inputCols = df.columns[:len(df.columns)-1],outputCol=\"features\")\n",
    "feature_vector= feature.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+--------------------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|            features|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+--------------------+\n",
      "| 63|  1|  3|     145| 233|  1|      0|    150|    0|    2.3|    0|  0|   1|     1|[63.0,1.0,3.0,145...|\n",
      "| 37|  1|  2|     130| 250|  0|      1|    187|    0|    3.5|    0|  0|   2|     1|[37.0,1.0,2.0,130...|\n",
      "| 41|  0|  1|     130| 204|  0|      0|    172|    0|    1.4|    2|  0|   2|     1|[41.0,0.0,1.0,130...|\n",
      "| 56|  1|  1|     120| 236|  0|      1|    178|    0|    0.8|    2|  0|   2|     1|[56.0,1.0,1.0,120...|\n",
      "| 57|  0|  0|     120| 354|  0|      1|    163|    1|    0.6|    2|  0|   2|     1|[57.0,0.0,0.0,120...|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_vector.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasDF = feature_vector.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector_select = feature_vector.select(['features','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, x_test) = feature_vector_select.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol='target',featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rf.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "|            features|target|       rawPrediction|         probability|prediction|\n",
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "|(13,[0,2,3,4,7,10...|     1|[1.14760664292285...|[0.05738033214614...|       1.0|\n",
      "|(13,[0,3,4,7,10,1...|     0|[0.98355852369041...|[0.04917792618452...|       1.0|\n",
      "|[34.0,0.0,1.0,118...|     1|[1.05972505180352...|[0.05298625259017...|       1.0|\n",
      "|[34.0,1.0,3.0,118...|     1|[2.91670752319277...|[0.14583537615963...|       1.0|\n",
      "|[37.0,0.0,2.0,120...|     1|[1.05972505180352...|[0.05298625259017...|       1.0|\n",
      "+--------------------+------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(x_test)\n",
    "prediction.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8676470588235294\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', MulticlassClassificationEvaluator(labelCol='target',predictionCol=\"prediction\",metricName='accuracy').evaluate(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "                                .addGrid(rf.maxDepth, [5, 10, 20]) \\\n",
    "                                .addGrid(rf.maxBins, [20, 32, 50]) \\\n",
    "                                .addGrid(rf.numTrees, [20, 40, 60 ]) \\\n",
    "                                .addGrid(rf.impurity, [\"gini\", \"entropy\"]) \\\n",
    "                                .addGrid(rf.minInstancesPerNode, [1, 5, 10]) \\\n",
    "                                .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o64752.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4243.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4243.0 (TID 4030) (DESKTOP-ONEK9CT executor driver): java.io.IOException: fail to rename file C:\\Users\\LENOVO\\AppData\\Local\\Temp\\blockmgr-40784e38-8652-41bc-ad5a-0b9ec51b9f58\\1a\\shuffle_1798_4030_0.data.d219e88a-a39c-4696-9ed6-be734edb9fb7 to C:\\Users\\LENOVO\\AppData\\Local\\Temp\\blockmgr-40784e38-8652-41bc-ad5a-0b9ec51b9f58\\1a\\shuffle_1798_4030_0.data\r\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeMetadataFileAndCommit(IndexShuffleBlockResolver.scala:385)\r\n\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.commitAllPartitions(LocalDiskShuffleMapOutputWriter.java:119)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:71)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:161)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\r\n\tat sun.reflect.GeneratedMethodAccessor162.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: fail to rename file C:\\Users\\LENOVO\\AppData\\Local\\Temp\\blockmgr-40784e38-8652-41bc-ad5a-0b9ec51b9f58\\1a\\shuffle_1798_4030_0.data.d219e88a-a39c-4696-9ed6-be734edb9fb7 to C:\\Users\\LENOVO\\AppData\\Local\\Temp\\blockmgr-40784e38-8652-41bc-ad5a-0b9ec51b9f58\\1a\\shuffle_1798_4030_0.data\r\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeMetadataFileAndCommit(IndexShuffleBlockResolver.scala:385)\r\n\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.commitAllPartitions(LocalDiskShuffleMapOutputWriter.java:119)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:71)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluation\u001b[39;00m \u001b[39mimport\u001b[39;00m MulticlassClassificationEvaluator\n\u001b[0;32m      2\u001b[0m tvs \u001b[39m=\u001b[39m TrainValidationSplit( estimator\u001b[39m=\u001b[39mrf\n\u001b[0;32m      3\u001b[0m                            ,estimatorParamMaps\u001b[39m=\u001b[39mparamGrid\n\u001b[0;32m      4\u001b[0m                            ,evaluator\u001b[39m=\u001b[39mMulticlassClassificationEvaluator(labelCol\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m                            ,trainRatio\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m tvs\u001b[39m.\u001b[39;49mfit(x_train)\n\u001b[0;32m      7\u001b[0m model_predictions\u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtransform(x_test)\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m'\u001b[39m, MulticlassClassificationEvaluator(labelCol\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m,predictionCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m,metricName\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mevaluate(model_predictions))\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\site-packages\\pyspark\\ml\\base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[0;32m    160\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\site-packages\\pyspark\\ml\\tuning.py:1225\u001b[0m, in \u001b[0;36mTrainValidationSplit._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m   1223\u001b[0m pool \u001b[39m=\u001b[39m ThreadPool(processes\u001b[39m=\u001b[39m\u001b[39mmin\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetParallelism(), numModels))\n\u001b[0;32m   1224\u001b[0m metrics \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m numModels\n\u001b[1;32m-> 1225\u001b[0m \u001b[39mfor\u001b[39;00m j, metric, subModel \u001b[39min\u001b[39;00m pool\u001b[39m.\u001b[39mimap_unordered(\u001b[39mlambda\u001b[39;00m f: f(), tasks):\n\u001b[0;32m   1226\u001b[0m     metrics[j] \u001b[39m=\u001b[39m metric\n\u001b[0;32m   1227\u001b[0m     \u001b[39mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\multiprocessing\\pool.py:870\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m    869\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[1;32m--> 870\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\multiprocessing\\pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    123\u001b[0m job, i, func, args, kwds \u001b[39m=\u001b[39m task\n\u001b[0;32m    124\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     result \u001b[39m=\u001b[39m (\u001b[39mTrue\u001b[39;00m, func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[0;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m wrap_exception \u001b[39mand\u001b[39;00m func \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\site-packages\\pyspark\\ml\\tuning.py:1225\u001b[0m, in \u001b[0;36mTrainValidationSplit._fit.<locals>.<lambda>\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m   1223\u001b[0m pool \u001b[39m=\u001b[39m ThreadPool(processes\u001b[39m=\u001b[39m\u001b[39mmin\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetParallelism(), numModels))\n\u001b[0;32m   1224\u001b[0m metrics \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m numModels\n\u001b[1;32m-> 1225\u001b[0m \u001b[39mfor\u001b[39;00m j, metric, subModel \u001b[39min\u001b[39;00m pool\u001b[39m.\u001b[39mimap_unordered(\u001b[39mlambda\u001b[39;00m f: f(), tasks):\n\u001b[0;32m   1226\u001b[0m     metrics[j] \u001b[39m=\u001b[39m metric\n\u001b[0;32m   1227\u001b[0m     \u001b[39mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\site-packages\\pyspark\\util.py:326\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[39m# Set local properties in child thread.\u001b[39;00m\n\u001b[0;32m    325\u001b[0m     SparkContext\u001b[39m.\u001b[39m_active_spark_context\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39msetLocalProperties(properties)\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    327\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    328\u001b[0m     InheritableThread\u001b[39m.\u001b[39m_clean_py4j_conn_for_current_thread()\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\site-packages\\pyspark\\ml\\tuning.py:69\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msingleTask\u001b[39m():\n\u001b[1;32m---> 69\u001b[0m     index, model \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(modelIter)\n\u001b[0;32m     70\u001b[0m     \u001b[39m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[39m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[39m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     \u001b[39m#  all nested stages and evaluators\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     metric \u001b[39m=\u001b[39m eva\u001b[39m.\u001b[39mevaluate(model\u001b[39m.\u001b[39mtransform(validation, epm[index]))\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\site-packages\\pyspark\\ml\\base.py:69\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo models remaining.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcounter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m index, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfitSingleModel(index)\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\site-packages\\pyspark\\ml\\base.py:126\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfitSingleModel\u001b[39m(index):\n\u001b[1;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator\u001b[39m.\u001b[39;49mfit(dataset, paramMaps[index])\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\site-packages\\pyspark\\ml\\base.py:159\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(params, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m params:\n\u001b[1;32m--> 159\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy(params)\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    160\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(dataset)\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\site-packages\\pyspark\\ml\\wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset):\n\u001b[1;32m--> 335\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[0;32m    336\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    337\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\site-packages\\pyspark\\ml\\wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[39mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39m    fitted Java model\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 332\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32me:\\PROJECTS\\Spark-Project\\venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o64752.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4243.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4243.0 (TID 4030) (DESKTOP-ONEK9CT executor driver): java.io.IOException: fail to rename file C:\\Users\\LENOVO\\AppData\\Local\\Temp\\blockmgr-40784e38-8652-41bc-ad5a-0b9ec51b9f58\\1a\\shuffle_1798_4030_0.data.d219e88a-a39c-4696-9ed6-be734edb9fb7 to C:\\Users\\LENOVO\\AppData\\Local\\Temp\\blockmgr-40784e38-8652-41bc-ad5a-0b9ec51b9f58\\1a\\shuffle_1798_4030_0.data\r\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeMetadataFileAndCommit(IndexShuffleBlockResolver.scala:385)\r\n\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.commitAllPartitions(LocalDiskShuffleMapOutputWriter.java:119)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:71)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:161)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\r\n\tat sun.reflect.GeneratedMethodAccessor162.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: fail to rename file C:\\Users\\LENOVO\\AppData\\Local\\Temp\\blockmgr-40784e38-8652-41bc-ad5a-0b9ec51b9f58\\1a\\shuffle_1798_4030_0.data.d219e88a-a39c-4696-9ed6-be734edb9fb7 to C:\\Users\\LENOVO\\AppData\\Local\\Temp\\blockmgr-40784e38-8652-41bc-ad5a-0b9ec51b9f58\\1a\\shuffle_1798_4030_0.data\r\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeMetadataFileAndCommit(IndexShuffleBlockResolver.scala:385)\r\n\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.commitAllPartitions(LocalDiskShuffleMapOutputWriter.java:119)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:71)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "tvs = TrainValidationSplit( estimator=rf\n",
    "                           ,estimatorParamMaps=paramGrid\n",
    "                           ,evaluator=MulticlassClassificationEvaluator(labelCol='target')\n",
    "                           ,trainRatio=0.8)\n",
    "model = tvs.fit(x_train)\n",
    "model_predictions= model.transform(x_test)\n",
    "\n",
    "\n",
    "print('Accuracy: ', MulticlassClassificationEvaluator(labelCol='target',predictionCol=\"prediction\",metricName='accuracy').evaluate(model_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d56a2e1583cc91001f51b4648654dfeeefc02c3b006ec9b840bc622a1ba33f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
